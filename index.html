<!DOCTYPE html>
<html lang="en">

<head>
    <title>Tyler Hayes</title>
    <description>Tyler Hayes Personal Website</description>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="tyler, hayes, rit, rochester institute of technology, new york, computer vision, deep-learning, machine-learning, machine learning, deep learning, artificial intelligence, AI, segmentation, image, graph, manifold, math, RIT, Rochester, rochester, lifelong learning, continual learning">
    <meta name="author" content="Tyler Hayes">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-151327506-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-151327506-1');
    </script>

    <script>
        /**
         * Function that registers a click on an outbound link in Analytics.
         * This function takes a valid URL string as an argument, and uses that URL string
         * as the event label. Setting the transport method to 'beacon' lets the hit be sent
         * using 'navigator.sendBeacon' in browser that support it.
         */
        var getOutboundLink = function(url) {
            gtag('event', 'click', {
                'event_category': 'outbound',
                'event_label': url,
                'transport_type': 'beacon',
                'event_callback': function() {
                    document.location = url;
                }
            });
        }
    </script>

    <style>
        .newflag {
            background-color: #C0B283;
            color: #ffffff;
            padding: 2px 5px;
            border-radius: 5px;
            border: 2px solid #C0B283;
            font: 600 16px Montserrat, sans-serif;
            font-family: Montserrat, sans-serif;
        }
    </style>

    <style>
        body {
            font: 400 15px Montserrat, sans-serif;
            font-family: Montserrat, sans-serif;
            line-height: 1.8;
            color: #818181;
        }
        
        img {
            width: 100%;
            height: auto;
        }
        
        h2 {
            font-size: 24px;
            text-transform: uppercase;
            font-weight: 600;
            margin-bottom: 20px;
            text-align: center;
            font-family: Montserrat, sans-serif;
            letter-spacing: 4px;
        }
        
        h3 {
            font-size: 22px;
            font-weight: 400;
            margin-bottom: 20px;
            text-align: left;
        }
        
        h4 {
            font-size: 19px;
            line-height: 1.375em;
            font-weight: 400;
            margin-bottom: 20px;
            text-align: left;
        }
        
        h5 {
            font-size: 16px;
            line-height: 1.375em;
            font-weight: 400;
            margin-bottom: 15px;
        }
        
        h6 {
            font-size: 15px;
            line-height: 1.375em;
            font-weight: 400;
            margin-bottom: 14px;
            text-align: right;
        }
        
        .jumbotron {
            background-image: url("images/bulbs.jpg");
            background-size: 142% auto;
            background-align: center;
            padding: 100px 25px;
            font-family: Montserrat, sans-serif;
        }
        
        .container-fluid {
            padding: 60px 50px;
        }
        
        .section-navbar {
            color: #C0B283;
            background-color: #FFFFFF;
        }
        
        .section-default {
            color: #373737;
            background-color: #F4F4F4;
        }
        
        .section-about-me,
        .section-about-me h4,
        .section-about-me a {
            color: #373737;
            background-color: #F4F4F4;
            text-align: justify;
        }
        
        .section-recent-news {
            color: #373737;
            background-color: #ede6de;
        }
        
        .section-research {
            color: #373737;
            background-color: #F4F4F4;
            text-align: justify;
        }
        
        .section-research h3 {
            text-align: center;
        }
        
        .section-research h4 {
            text-align: center;
        }
        
        .section-publications,
        .section-publications a {
            color: #373737;
            background-color: #ede6de;
            text-align: justify;
        }
        
        .logo {
            color: #f4511e;
            font-size: 200px;
        }
        
        .thumbnail {
            padding: 0 0 15px 0;
            border: none;
            border-radius: 0;
        }
        
        .thumbnail2 {
            width: 60px;
            height: auto;
            margin-bottom: 10px;
        }
        
        .carousel-control.right,
        .carousel-control.left {
            background-image: none;
            color: #f4511e;
        }
        
        .carousel-indicators li {
            border-color: #f4511e;
        }
        
        .carousel-indicators li.active {
            background-color: #f4511e;
        }
        
        .item h4 {
            font-size: 19px;
            line-height: 1.375em;
            font-weight: 400;
            font-style: italic;
            margin: 70px 0;
        }
        
        .item span {
            font-style: normal;
        }
        
        .panel {
            border: 1px solid #f4511e;
            border-radius: 0 !important;
            transition: box-shadow 0.5s;
        }
        
        .panel:hover {
            box-shadow: 5px 0px 40px rgba(0, 0, 0, .2);
        }
        
        .panel-footer .btn:hover {
            border: 1px solid #f4511e;
            background-color: #fff !important;
            color: #f4511e;
        }
        
        .panel-heading {
            color: #fff !important;
            background-color: #f4511e !important;
            padding: 25px;
            border-bottom: 1px solid transparent;
            border-top-left-radius: 0px;
            border-top-right-radius: 0px;
            border-bottom-left-radius: 0px;
            border-bottom-right-radius: 0px;
        }
        
        .panel-footer {
            background-color: white !important;
        }
        
        .panel-footer h3 {
            font-size: 32px;
            font-weight: 600;
            margin-bottom: 30px;
        }
        
        .panel-footer h4 {
            font-size: 14px;
        }
        
        .panel-footer .btn {
            margin: 15px 0;
        }
        
        .navbar {
            margin-bottom: 0;
            z-index: 9999;
            border: 0;
            font-size: 12px !important;
            line-height: 1.42857143 !important;
            letter-spacing: 4px;
            border-radius: 0;
            font-family: Montserrat, sans-serif;
        }
        
        .navbar li a,
        .navbar .navbar-brand {
            font-family: Montserrat, sans-serif;
        }
        
        .navbar-nav li a:hover,
        .navbar-nav li.active a {
            background-color: #C0B283 !important;
            color: #FFFFFF !important;
            transition: 0.3s;
        }
        
        .navbar-default .navbar-toggle {
            border-color: transparent;
            color: #fff !important;
        }
        
        footer {
            background-color: #ffffff;
        }
        
        footer .glyphicon {
            font-size: 20px;
            margin-bottom: 20px;
            color: #C0B283;
        }
        
        .slideanim {
            visibility: hidden;
        }
        
        .slide {
            animation-name: slide;
            -webkit-animation-name: slide;
            animation-duration: 1s;
            -webkit-animation-duration: 1s;
            visibility: visible;
        }
        
        @keyframes slide {
            0% {
                opacity: 0;
                transform: translateY(70%);
            }
            100% {
                opacity: 1;
                transform: translateY(0%);
            }
        }
        
        @-webkit-keyframes slide {
            0% {
                opacity: 0;
                -webkit-transform: translateY(70%);
            }
            100% {
                opacity: 1;
                -webkit-transform: translateY(0%);
            }
        }
        
        @media screen and (max-width: 768px) {
            .col-sm-4 {
                text-align: center;
                margin: 25px 0;
            }
            .btn-lg {
                width: 100%;
                margin-bottom: 35px;
            }
        }
        
        @media screen and (max-width: 480px) {
            .logo {
                font-size: 150px;
            }
        }
        
        .external-link-icon {
            float: left;
            margin: 0pt 5pt;
            width: 50pt;
            height: 50pt;
            opacity: 1;
        }
        
        .external-link-icon img {
            transition: 0.3s;
            border-radius: 50%;
        }
        
        .external-link-icon img:hover {
            background-color: #C0B283;
            transition: 0.3s;
            border-radius: 50%;
        }
        
        .external-link-container {
            text-align: center;
            width: 100%;
            height: 60pt;
        }
        
        .external-link-center-wrapper {
            margin: 20px auto auto;
            width: 300pt;
            height: 60pt;
        }
        
        .button {
            background-color: #373737;
            border: none;
            color: white;
            padding: 8px 32px;
            text-align: center;
            font-size: 18px;
            margin: 4px 2px;
            transition: 0.3s;
            display: inline-block;
            text-decoration: none;
            cursor: pointer;
            border-radius: 5px;
        }
        
        .button:hover {
            background-color: #C0B283;
            color: #ffffff;
        }
    </style>
</head>

<body id="myPage" data-spy="scroll" data-target=".navbar" data-offset="60" class="section-default">

    <nav class="navbar navbar-default navbar-fixed-top section-navbar">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="#myPage">Tyler Hayes</a>
            </div>
            <div class="collapse navbar-collapse" id="myNavbar">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="#about">ABOUT</a></li>
                    <li><a href="#recent">RECENT</a></li>
                    <li><a href="#research">RESEARCH</a></li>
                    <li><a href="#publications">PUBLICATIONS</a></li>
                    <li data-toggle="collapse" data-target=".navbar-collapse.in"><a href="data/Tyler_Hayes_CV.pdf" onclick="getOutboundLink('data/Tyler_Hayes_CV.pdf'); return false;" target="_blank">CV</a></li>
                    </h1>
            </div>
        </div>
    </nav>

    <div class="jumbotron text-center">
        <h1>Tyler Hayes<br>
  	<div class = "external-link-container">
	  <div class="external-link-center-wrapper">
	    <div class = "external-link-icon">
	    	<a href="https://www.linkedin.com/in/tyler-hayes-6a91a461/" onclick="getOutboundLink('https://www.linkedin.com/in/tyler-hayes-6a91a461/'); return false;" target="_blank">
            	<span><img border="0" alt="Tyler Hayes Linkedin" src="images/linkedin-logo.png"></img></span>
            </a>
	    </div>
	    <div class = "external-link-icon">
	    	<a href="https://scholar.google.com/citations?user=LZ1PWfcAAAAJ&hl=en#" onclick="getOutboundLink('https://scholar.google.com/citations?user=LZ1PWfcAAAAJ&hl=en#'); return false;" target="_blank">
            	<span><img border="0" alt="Tyler Hayes Google Scholar" src="images/gs-logo.png"></img></span>
            </a>
	    </div>
		<div class = "external-link-icon">
	    	<a href="https://github.com/tyler-hayes" onclick="getOutboundLink('https://github.com/tyler-hayes'); return false;" target="_blank">
				<span><img border="0" alt="Tyler Hayes GitHub" src="images/github-logo.png"></img></span>
            </a>
	    </div>
		<div class = "external-link-icon">
	    	<a href="https://twitter.com/tylerlhayes" onclick="getOutboundLink('https://twitter.com/tylerlhayes'); return false;" target="_blank">
				<span><img border="0" alt="Tyler Hayes Twitter" src="images/twitter-logo.png"></img></span>
            </a>
	    </div>
	    <div class = "external-link-icon">
	    	<a href="" rel="nofollow" onclick="this.href='mailto:' + 'tlh6792' + '@' + 'rit.edu'" target="_blank">
            	<span><img border="0" alt="Tyler Hayes Contact" src="images/mail-logo.png"></img></span>
            </a>
	    </div>
	  </div>
	</div>
</div>

  </form>

<!-- Container (About Section) -->
<div id="about" class="container-fluid section-about-me">
  <div class="row">
    <h2>About Me</h2>
    <div class="col-sm-6 col-sm-offset-2">
      <h4>Hello! I am a fourth year PhD student in the <a href="https://www.cis.rit.edu/" onclick="getOutboundLink('https://www.cis.rit.edu/'); return false;">Chester F. Carlson Center for Imaging Science</a> at the <a href="https://www.rit.edu/" onclick="getOutboundLink('https://www.rit.edu/'); return false;">Rochester Institute of Technology (RIT)</a> in Rochester, NY. I currently work in the <a href="http://klab.cis.rit.edu/" onclick="getOutboundLink('http://klab.cis.rit.edu/'); return false;"> Machine and Neuromorphic Perception Laboratory (a.k.a. kLab)</a> under the direction of my advisor, <a href="https://chriskanan.com/" onclick="getOutboundLink('https://chriskanan.com/'); return false;" >Dr. Christopher Kanan</a>. My current research interests include lifelong machine learning, computer vision, and computational mathematics.
Previously, I earned a BS in Applied Mathematics from RIT in 2014 and an MS in Applied and Computational Mathematics from RIT in 2017. I have an <a href="https://www.csauthors.net/distance/tyler-l-hayes/paul-erdos" onclick="getOutboundLink('https://www.csauthors.net/distance/tyler-l-hayes/paul-erdos'); return false;">Erdős number</a> of 4!
</p>
    </div>
    <div class="col-sm-2">
      <span><img src="images/tyler.jpg" alt="Tyler Hayes" class="img-responsive" style="border-radius: 25px"></span>
	</div>
  </div>
</div>

<!-- Container (Recent News) -->
<div id="recent" class="container-fluid section-recent-news">
  <div class="row">
    <div class="col-sm-8 col-sm-offset-2">
      <h2>Recent News</h2>
	  <h4><strong>Jul 2020:</strong> <span class="newflag">NEW!</span> Our paper <a href="https://arxiv.org/abs/1910.02509" onclick="getOutboundLink('https://arxiv.org/abs/1910.02509'); return false;">"REMIND Your Neural Network to Prevent Catastrophic Forgetting"</a> was accepted for poster presentation at ECCV 2020! (27.1% acceptance rate)</h4>
	  <h4><strong>Jun 2020:</strong> <span class="newflag">NEW!</span> Our paper on Deep SLDA won the <a href="https://sites.google.com/view/clvision2020/paper-awards?authuser=0" onclick="getOutboundLink('https://sites.google.com/view/clvision2020/paper-awards?authuser=0'); return false;">Best Paper Award</a> at the CVPR 2020 Workshop on Continual Learning!</h4>
	  <h4><strong>May 2020:</strong> Gave an invited talk at the <a href="https://www.continualai.org/" onclick="getOutboundLink('https://www.continualai.org/'); return false;">Continual AI</a> Meetup on <a href="https://www.youtube.com/watch?v=Qo2JKIDZz6w" onclick="getOutboundLink('https://www.youtube.com/watch?v=Qo2JKIDZz6w'); return false;">Continual Learning with Sequential Streaming Data</a>!</h4>
	  <h4><strong>May 2020:</strong> Won a travel grant to attend the CVPR Workshop on Women in Computer Vision (WiCV)!</h4>
			<h4><strong>Apr 2020:</strong> My research journey was featured in an <a href="https://www.rit.edu/science/news/student-student-artificial-intelligencemachine-learning?fbclid=IwAR1SMfMZA6ryy8C31YATz5CHOOfDnybFgdnfnfyMO74wYGXod7pqXzJcE9I" onclick="getOutboundLink('https://www.rit.edu/science/news/student-student-artificial-intelligencemachine-learning?fbclid=IwAR1SMfMZA6ryy8C31YATz5CHOOfDnybFgdnfnfyMO74wYGXod7pqXzJcE9I'); return false;">RIT News article</a>!</h4>
			<h4><strong>Apr 2020:</strong> Our extended abstract <a href="https://arxiv.org/abs/1910.02509" onclick="getOutboundLink('https://arxiv.org/abs/1910.02509'); return false;">"REMIND Your Neural Network to Prevent Catastrophic Forgetting"</a> was accepted to the CVPR 2020 Workshop on Women in Computer Vision (WiCV)! </h4>
			<h4><strong>Apr 2020:</strong> Our paper <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html'); return false;">"Stream-51: Streaming Classification and Novelty Detection from Videos"</a> was accepted for poster presentation at the CVPR 2020 Workshop on Continual Learning! See our project webpage <a href="https://tyler-hayes.github.io/stream51" onclick="getOutboundLink('https://tyler-hayes.github.io/stream51'); return false;">here</a>.</h4>
			<h4><strong>Apr 2020:</strong> Our paper <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html'); return false;">"Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis"</a> was accepted for oral presentation at the CVPR 2020 Workshop on Continual Learning!</h4>
			<h4><strong>Apr 2019:</strong> Gave an invited talk at the RIT <a href="https://www.rit.edu/chai/" onclick="getOutboundLink('https://www.rit.edu/chai/'); return false;">CHAI</a> AI Seminar Series on "Memory Efficient Experience Replay for Mitigating Catastrophic Forgetting."</h4>	
	       <h4><strong>Jan 2019:</strong> Our paper <a href="https://ieeexplore.ieee.org/document/8793982" onclick="getOutboundLink('https://ieeexplore.ieee.org/document/8793982'); return false;">"Memory Efficient Experience Replay for Streaming Learning"</a> was accepted for poster presentation at ICRA 2019! (44.0% acceptance rate)</h4>
           <h4><strong>Feb 2018:</strong> Our paper <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html'); return false;">"Compassionately Conservative Balanced Cuts for Image Segmentation"</a> was accepted for poster presentation at CVPR 2018! (29.6% acceptance rate)</h4>
           <h4><strong>Nov 2017:</strong> Our paper <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410" onclick="getOutboundLink('https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410'); return false;">"Measuring Catastrophic Forgetting in Neural Networks"</a> was accepted for a spotlight presentation at AAAI 2018! (24.6% acceptance
rate)</h4>
           <h4><strong>Jun 2017:</strong> Passed my PhD Qualification Exam.</h4>		   
           <h4><strong>Jun 2017:</strong> Started work as a Research Intern in the <a href="https://www.nrl.navy.mil/itd/aic/" onclick="getOutboundLink('https://www.nrl.navy.mil/itd/aic/'); return false;">Navy Center for Applied Research in Artificial Intelligence</a> at the <a href="https://www.nrl.navy.mil/" onclick="getOutboundLink('https://www.nrl.navy.mil/'); return false;">US Naval Research Laboratory (NRL)</a>.</h4>
           <h4><strong>Mar 2017:</strong> Successfully defended my MS thesis.</h4>
    </div>
  </div>
</div>

<!-- Container (Research Section) -->
<div id="research" class="container-fluid section-research">
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h2>Research</h2>
		</div>
	</div>

	<!-- ECCV 2020 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>ECCV 2020</strong>: REMIND Your Neural Network to Prevent Catastrophic Forgetting</h3>
			<h4><strong>Tyler L. Hayes*</strong>, Kushal Kafle*, Robik Shrestha*, Manoj Acharya, Christopher Kanan</h4>
			<h6>* denotes equal contribution.</h6>
			<h3>
				<a href="https://arxiv.org/abs/1910.02509" onclick="getOutboundLink('https://arxiv.org/abs/1910.02509'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://github.com/tyler-hayes/REMIND" onclick="getOutboundLink('https://github.com/tyler-hayes/REMIND'); return false;" target="_blank"><button class="button">Code</button></a>
				<a href="https://www.youtube.com/watch?v=Qo2JKIDZz6w" onclick="getOutboundLink('https://www.youtube.com/watch?v=Qo2JKIDZz6w'); return false;" target="_blank"><button class="button">Full Talk</button></a>
				<a href="https://www.youtube.com/watch?v=xA5UMOdu8y8" onclick="getOutboundLink('https://www.youtube.com/watch?v=xA5UMOdu8y8'); return false;" target="_blank"><button class="button">Video</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-3 col-sm-offset-2">
			<img src="images/arxiv_2019_remind.png" alt="arXiv 2019" class="img-responsive"></img>
		</div>
		<div class="col-sm-5">
			<h5 class="h5-small"><p>People learn throughout life. However, incrementally updating conventional neural networks leads to catastrophic forgetting. A common remedy is replay, which is inspired by how the brain consolidates memory. Replay involves fine-tuning a network on a mixture of new and old instances. While the brain replays compressed memories, existing methods for convolutional networks replay raw images. Here, we propose REMIND, a brain-inspired approach that enables efficient replay with compressed hidden representations. REMIND is trained in an online manner, meaning it learns one example at a time, which is closer to how humans perceive new information. Under the same constraints, REMIND outperforms other methods for incremental class learning on the ImageNet ILSVRC-2012 dataset. We probe REMIND's robustness to data ordering schemes known to induce catastrophic forgetting. We demonstrate REMIND's generality by pioneering online learning for Visual Question Answering (VQA), which cannot be readily done with comparison models.</p></h5>
		</div>
    </div>

	<!-- CVPRW 2020 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>CVPRW 2020</strong>: Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis</h3>
			<h4><strong>Tyler L. Hayes</strong> & Christopher Kanan</h4>
			<h4><strong>Best Paper Award at the CVPR 2020 Workshop on Continual Learning in Computer Vision</strong></h4>
			<h3>
				<a href="https://arxiv.org/abs/1909.01520" onclick="getOutboundLink('https://arxiv.org/abs/1909.01520'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html'); return false;" target="_blank"><button class="button">Paper</button></a>
				<a href="https://github.com/tyler-hayes/Deep_SLDA" onclick="getOutboundLink('https://github.com/tyler-hayes/Deep_SLDA'); return false;" target="_blank"><button class="button">Code</button></a>
				<a href="data/Deep_SLDA_Poster.pdf" onclick="getOutboundLink('data/Deep_SLDA_Poster.pdf'); return false;" target="_blank"><button class="button">Poster</button></a>
				<a href="https://www.youtube.com/watch?v=Nklyx-AElwo" onclick="getOutboundLink('https://www.youtube.com/watch?v=Nklyx-AElwo'); return false;" target="_blank"><button class="button">Video</button></a>
			</h3>
		</div>
	</div>
	
    <div class="row">
		<div class="col-sm-3 col-sm-offset-2">
			<img src="images/cvprw_2020_slda.png" alt="arXiv 2019" class="img-responsive"></img>
		</div>
		<div class="col-sm-5">
			<h5 class="h5-small"><p>When an agent acquires new information, ideally it would immediately be capable of using that information to understand its environment. This is not possible using conventional deep neural networks, which suffer from catastrophic forgetting when they are incrementally updated, with new knowledge overwriting established representations. A variety of approaches have been developed that attempt to mitigate catastrophic forgetting in the incremental batch learning scenario, where a model learns from a series of large collections of labeled samples. However, in this setting, inference is only possible after a batch has been accumulated, which prohibits many applications. An alternative paradigm is online learning in a single pass through the training dataset on a resource constrained budget, which is known as streaming learning. Streaming learning has been much less studied in the deep learning community. In streaming learning, an agent learns instances one-by-one and can be tested at any time, rather than only after learning a large batch. Here, we revisit streaming linear discriminant analysis, which has been widely used in the data mining research community. By combining streaming linear discriminant analysis with deep learning, we are able to outperform both incremental batch learning and streaming learning algorithms on both ImageNet ILSVRC-2012 and CORe50, a dataset that involves learning to classify from temporally ordered samples.</p></h5>
		</div>
    </div>
	
	<!-- CVPRW 2020 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>CVPRW 2020</strong>: Stream-51: Streaming Classification and Novelty Detection from Videos</h3>
			<h4>Ryne Roady*, <strong>Tyler L. Hayes*</strong>, Hitesh Vaidya, & Christopher Kanan</h4>
			<h6>* denotes equal contribution.</h6>
			<h3>
				<a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html'); return false;" target="_blank"><button class="button">Paper</button></a>
				<a href="https://tyler-hayes.github.io/stream51" onclick="getOutboundLink('https://tyler-hayes.github.io/stream51'); return false;" target="_blank"><button class="button">Project Page</button></a>
				<a href="https://github.com/tyler-hayes/Stream-51" onclick="getOutboundLink('https://github.com/tyler-hayes/Stream-51'); return false;" target="_blank"><button class="button">Code</button></a>
				<a href="https://www.youtube.com/watch?v=cRtRTcWqhsU" onclick="getOutboundLink('https://www.youtube.com/watch?v=cRtRTcWqhsU'); return false;" target="_blank"><button class="button">Video</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-3 col-sm-offset-2">
			<img src="images/cvprw_2020_stream51.png" alt="arXiv 2019" class="img-responsive"></img>
		</div>
		<div class="col-sm-5">
			<h5 class="h5-small"><p>Deep neural networks are popular for visual perception tasks such as image classification and object detection. Once trained and deployed in a real-time environment, these models struggle to identify novel inputs not initially represented in the training distribution. Further, they cannot be easily updated on new information or they will catastrophically forget previously learned knowledge. While there has been much interest in developing models capable of overcoming forgetting, most research has focused on incrementally learning from common image classification datasets broken up into large batches. Online streaming learning is a more realistic paradigm where a model must learn one sample at a time from temporally correlated data streams. Although there are a few datasets designed specifically for this protocol, most have limitations such as few classes or poor image quality. In this work, we introduce Stream-51, a new dataset for streaming classification consisting of temporally correlated images from 51 distinct object categories and additional evaluation classes outside of the training distribution to test novelty recognition. We establish unique evaluation protocols, experimental metrics, and baselines for our dataset in the streaming paradigm.</p></h5>
		</div>
    </div>
	
	<!-- arXiv Preprint 2020 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>arXiv 2020</strong>: Do We Need Fully Connected Output Layers in Convolutional Networks?</h3>
			<h4>Zhongchao Qian, <strong>Tyler L. Hayes</strong>, Kushal Kafle, Christopher Kanan</h4>
			<h3>
				<a href="http://arxiv.org/abs/2004.13587" onclick="getOutboundLink('http://arxiv.org/abs/2004.13587'); return false;" target="_blank"><button class="button">arXiv</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-3 col-sm-offset-2">
			<img src="images/arxiv_2019_fixed_output.png" alt="arXiv 2019" class="img-responsive"></img>
		</div>
		<div class="col-sm-5">
			<h5 class="h5-small"><p>Traditionally, deep convolutional neural networks consist of a series of convolutional and pooling layers followed by one or more fully connected (FC) layers to perform the final classification. While this design has been successful, for datasets with a large number of categories, the fully connected layers often account for a large percentage of the network's parameters. For applications with memory constraints, such as mobile devices and embedded platforms, this is not ideal. Recently, a family of architectures that involve replacing the learned fully connected output layer with a fixed layer has been proposed as a way to achieve better efficiency. In this paper we examine this idea further and demonstrate that fixed classifiers offer no additional benefit compared to simply removing the output layer along with its parameters. We further demonstrate that the typical approach of having a fully connected final output layer is inefficient in terms of parameter count. We are able to achieve comparable performance to a traditionally learned fully connected classification output layer on the ImageNet-1K, CIFAR-100, Stanford Cars-196, and Oxford Flowers-102 datasets, while not having a fully connected output layer at all.</p></h5>
		</div>
    </div>

		<!-- arXiv Preprint 2019 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>arXiv 2019</strong>: Are Out-of-Distribution Detection Methods Effective on Large-Scale Datasets?</h3>
			<h4>Ryne Roady, <strong>Tyler L. Hayes</strong>, Ronald Kemker, Ayesha Gonzales, Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/1910.14034" onclick="getOutboundLink('https://arxiv.org/abs/1910.14034'); return false;" target="_blank"><button class="button">arXiv</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-3 col-sm-offset-2">
			<img src="images/arxiv_2019_ood.PNG" alt="arXiv 2019" class="img-responsive"></img>
		</div>
		<div class="col-sm-5">
			<h5 class="h5-small"><p>Supervised classification methods often assume the train and test data distributions are the same and that all classes in the test set are present in the training set. However, deployed classifiers often require the ability to recognize inputs from outside the training set as unknowns. This problem has been studied under multiple paradigms including out-of-distribution detection and open set recognition. For convolutional neural networks, there have been two major approaches: 1) inference methods to separate knowns from unknowns and 2) feature space regularization strategies to improve model robustness to outlier inputs. There has been little effort to explore the relationship between the two approaches and directly compare performance on anything other than small-scale datasets that have at most 100 categories. Using ImageNet-1K and Places-434, we identify novel combinations of regularization and specialized inference methods that perform best across multiple outlier detection problems of increasing difficulty level. We found that input perturbation and temperature scaling yield the best performance on large scale datasets regardless of the feature space regularization strategy. Improving the feature space by regularizing against a background class can be helpful if an appropriate background class can be found, but this is impractical for large scale image classification datasets.</p></h5>
		</div>
    </div>

    <!-- ICRA 2019 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>ICRA 2019</strong>: Memory Efficient Experience Replay for Streaming Learning</h3>
			<h4><strong>Tyler L. Hayes</strong>, Nathan D. Cahill, Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/1809.05922" onclick="getOutboundLink('https://arxiv.org/abs/1809.05922'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://ieeexplore.ieee.org/document/8793982" onclick="getOutboundLink('https://ieeexplore.ieee.org/document/8793982'); return false;" target="_blank"><button class="button">Paper</button></a>
				<a href="https://github.com/tyler-hayes/ExStream" onclick="getOutboundLink('https://github.com/tyler-hayes/ExStream'); return false;" target="_blank"><button class="button">Code</button></a>
				<a href="data/ICRA2019_Hayes_Poster.pdf" onclick="getOutboundLink('data/ICRA2019_Hayes_Poster.pdf'); return false;" target="_blank"><button class="button">Poster</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-3 col-sm-offset-2">
			<img src="images/icra_2019.png" alt="ICRA 2019" class="img-responsive"></img>
		</div>
		<div class="col-sm-5">
			<h5 class="h5-small"><p>In supervised machine learning, an agent is typically trained once and then deployed. While this works well for static settings, robots often operate in changing environments and must quickly learn new things from data streams. In this paradigm, known as streaming learning, a learner is trained online, in a single pass, from a data stream that cannot be assumed to be independent and identically distributed (iid). Streaming learning will cause conventional deep neural networks (DNNs) to fail for two reasons: 1) they need multiple passes through the entire dataset; and 2) non-iid data will cause catastrophic forgetting. An old fix to both of these issues is rehearsal. To learn a new example, rehearsal mixes it with previous examples, and then this mixture is used to update the DNN. Full rehearsal is slow and memory intensive because it stores all previously observed examples, and its effectiveness for preventing catastrophic forgetting has not been studied in modern DNNs. Here, we describe the ExStream algorithm for memory efficient rehearsal and compare it to alternatives. We find that full rehearsal can eliminate catastrophic forgetting in a variety of streaming learning settings, with ExStream performing well using far less memory and computation.</p></h5>
		</div>
    </div>

	<!-- CVPRW 2018 -->
			</ul></h5>
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>CVPRW 2018</strong>: New Metrics and Experimental Paradigms for Continual Learning</h3>
			<h4><strong>Tyler L. Hayes</strong>, Ronald Kemker, Nathan D. Cahill, Christopher Kanan</h4>
			<h3>
				<a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w40/html/Hayes_New_Metrics_and_CVPR_2018_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_cvpr_2018_workshops/w40/html/Hayes_New_Metrics_and_CVPR_2018_paper.html'); return false;" target="_blank"><button class="button">Paper</button></a>
				<a href="data/cvpr18_workshop_poster_final.pdf" onclick="getOutboundLink('data/cvpr18_workshop_poster_final.pdf'); return false;" target="_blank"><button class="button">Poster</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-3 col-sm-offset-2">
			<img src="images/cvprw_2018.png" alt="CVPRW 2018" class="img-responsive"></img>
		</div>
		<div class="col-sm-5">
			<h5 class="h5-small"><p>In order for a robotic agent to learn successfully in an uncontrolled environment, it must be able to immediately alter its behavior. Deep neural networks are the dominant approach for classification tasks in computer vision, but typical algorithms and architectures are incapable of immediately learning new tasks without catastrophically forgetting previously acquired knowledge. There has been renewed interest in solving this problem, but there are limitations to existing solutions, including poor performance compared to offline models, large memory footprints, and learning slowly. In this abstract, we formalize the continual learning paradigm and propose new benchmarks for assessing continual learning agents.
</p></h5>
		</div>
    </div>

	<!-- CVPR 2018 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>CVPR 2018</strong>: Compassionately Conservative Balanced Cuts for Image Segmentation</h3>
			<h4>Nathan D. Cahill, <strong>Tyler L. Hayes</strong>, Renee T. Meinhold, John F. Hamilton</h4>
			<h3>
				<a href="https://arxiv.org/abs/1803.09903" onclick="getOutboundLink('https://arxiv.org/abs/1803.09903'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html'); return false;" target="_blank"><button class="button">Paper</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-3 col-sm-offset-2">
			<img src="images/cvpr_2018.PNG" alt="CVPR 2018" class="img-responsive"></img>
		</div>
		<div class="col-sm-5">
			<h5 class="h5-small"><p>The Normalized Cut (NCut) objective function, widely used in data clustering and image segmentation, quantifies the cost of graph partitioning in a way that biases clusters or segments that are balanced towards having lower values than unbalanced partitionings. However, this bias is so strong that it avoids any singleton partitions, even when vertices are very weakly connected to the rest of the graph. Motivated by the Buhler-Hein family of balanced cut costs, we propose the family of Compassionately Conservative Balanced (CCB) Cut costs, which are indexed by a parameter that can be used to strike a compromise between the desire to avoid too many singleton partitions and the notion that all partitions should be balanced. We show that CCB-Cut minimization can be relaxed into an orthogonally constrained lτ -minimization problem that coincides with the problem of computing Piecewise Flat Embeddings (PFE) for one particular index value, and we present an algorithm for solving the relaxed problem by iteratively minimizing a sequence of reweighted Rayleigh quotients (IRRQ). Using images from the BSDS500 database, we show that image segmentation based on CCB-Cut minimization provides better accuracy with respect to ground truth and greater variability in region size than NCut-based image segmentation.</p></h5>
		</div>
    </div>

     <!-- AAAI 2018 -->
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h3><strong>AAAI 2018</strong>: Measuring Catastrophic Forgetting in Neural Networks</h3>
			<h4>Ronald Kemker, Angelina Abitino, Marc McClure, <strong>Tyler L. Hayes</strong>, Christopher Kanan</h4>
			<h3>
				<a href="https://arxiv.org/abs/1708.02072" onclick="getOutboundLink('https://arxiv.org/abs/1708.02072'); return false;" target="_blank"><button class="button">arXiv</button></a>
				<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410" onclick="getOutboundLink('https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410'); return false;" target="_blank"><button class="button">Paper</button></a>
			</h3>
		</div>
	</div>

    <div class="row">
		<div class="col-sm-3 col-sm-offset-2">
			<img src="images/aaai_2018.png" alt="AAAI 2018" class="img-responsive"></img>
		</div>
		<div class="col-sm-5">
			<h5 class="h5-small"><p>Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than retraining the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem has yet to be solved.</p></h5>
		</div>
    </div>
</div>

<!-- Container (Publications) -->
<div id="publications" class="container-fluid section-publications">
	<div class="row">
		<div class="col-sm-8 col-sm-offset-2">
			<h2>Publications</h2>
			<h4>Peer-Reviewed Papers</h4>
			<h5><ul style="list-style-type:circle">
				<li><strong>T.L. Hayes*</strong>, K. Kafle*, R. Shrestha*, M. Acharya, and C. Kanan. <a href="https://arxiv.org/abs/1910.02509" onclick="getOutboundLink('https://arxiv.org/abs/1910.02509'); return false;">REMIND your neural network to prevent catastrophic forgetting</a>. In: Proc. European Conference on Computer Vision (ECCV), 2020</li>
				<li><strong>T.L. Hayes</strong> and C. Kanan. <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.html'); return false;">Lifelong machine learning with deep streaming linear discriminant analysis</a>. CVPR Workshop: Continual Learning in Computer Vision, 2020</li>
				<li>R. Roady*, <strong>T.L. Hayes*</strong>, H. Vaidya, and C. Kanan. <a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html'); return false;">Stream-51: Streaming Classification and Novelty Detection from Videos</a>. CVPR Workshop: Continual Learning in Computer Vision, 2020</li>
				<li><strong>T.L. Hayes</strong>, N.D. Cahill, and C. Kanan. <a href="https://ieeexplore.ieee.org/document/8793982" onclick="getOutboundLink('https://ieeexplore.ieee.org/document/8793982'); return false;">Memory efficient experience replay for streaming learning</a>. In: Proc. IEEE International Conference on Robotics and Automation (ICRA), 2019</li>
				<li><strong>T.L. Hayes</strong>, R. Kemker, N.D. Cahill, and C. Kanan. <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w40/html/Hayes_New_Metrics_and_CVPR_2018_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_cvpr_2018_workshops/w40/html/Hayes_New_Metrics_and_CVPR_2018_paper.html'); return false;">New metrics and experimental paradigms for continual learning</a>. CVPR Workshop: Real-World Challenges and New Benchmarks for Deep Learning in Robotic Vision, 2018</li>
				<li>N.D. Cahill, <strong>T.L. Hayes</strong>, R.T. Meinhold, and J.F. Hamilton. <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html" onclick="getOutboundLink('http://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html'); return false;">Compassionately conservative balanced cuts for image segmentation</a>. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018</li>
				<li>R. Kemker, M. McClure, A. Abitino, <strong>T.L. Hayes</strong>, and C. Kanan. <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410" onclick="getOutboundLink('https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410'); return false;">Measuring catastrophic forgetting in neural networks</a>. In: AAAI, 2018</li>
			</ul></h5>

			<h4>arXiv Pre-Prints</h4>
			<h5><ul style="list-style-type:circle">
			<li>Z. Qian, <strong>T.L. Hayes</strong>, K. Kafle, and C. Kanan. <a href="https://arxiv.org/abs/2004.13587" onclick="getOutboundLink('https://arxiv.org/abs/2004.13587'); return false;">Do we need fully connected output layers in convolutional networks?</a>. 2020</li>
			<li>R. Roady, <strong>T.L. Hayes</strong>, R. Kemker, A. Gonzales, and C. Kanan. <a href="https://arxiv.org/abs/1910.14034" onclick="getOutboundLink('https://arxiv.org/abs/1910.14034'); return false;">Are out-of-distribution detection methods effective on large-scale datasets?</a>. 2019</li>
			</ul></h5>

			<h4>Conference Papers</h4>
			<h5><ul style="list-style-type:circle">
				<li><strong>T.L. Hayes</strong>, R.T. Meinhold, J.F. Hamilton, and N.D. Cahill. <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10198/101980O/Piecewise-flat-embeddings-for-hyperspectral-image-analysis/10.1117/12.2262302.full" onclick="getOutboundLink('https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10198/101980O/Piecewise-flat-embeddings-for-hyperspectral-image-analysis/10.1117/12.2262302.full'); return false;">Piecewise flat embeddings for hyperspectral image analysis</a>. In: Proc. SPIE DCS Defense and Security: Algorithms and Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XXIII, 2017</li>
				<li>R.T. Meinhold, <strong>T.L. Hayes</strong>, and N.D. Cahill. <a href="https://arxiv.org/abs/1612.06496" onclick="getOutboundLink('https://arxiv.org/abs/1612.06496'); return false;">Efficiently computing piecewise flat embeddings for data clustering and image segmentation</a>. In: Proc. IEEE MIT Undergraduate Research and Technology Conference, 2016</li>
			</ul></h5>

			<h4>Thesis</h4>
			<h5><ul style="list-style-type:circle">
				<li><strong>T.L. Hayes</strong>. <a href="http://scholarworks.rit.edu/theses/9409/" onclick="getOutboundLink('http://scholarworks.rit.edu/theses/9409/'); return false;">Compassionately conservative normalized cuts for image segmentation</a>. M.S. Thesis, Rochester Institute of Technology, 2017</li>
			</ul></h5>
		</div>
	</div>
</div>

<footer class="container-fluid text-center">
  <a href="#myPage" title="To Top">
    <span class="glyphicon glyphicon-chevron-up"></span>
  </a>
</footer>

<script>
$(document).ready(function(){
  // Add smooth scrolling to all links in navbar + footer link
  $(".navbar a, footer a[href='#myPage']").on('click', function(event) {
    // Make sure this.hash has a value before overriding default behavior
    if (this.hash !== "") {
      // Prevent default anchor click behavior
      event.preventDefault();

      // Store hash
      var hash = this.hash;

      // Using jQuery's animate() method to add smooth page scroll
      // The optional number (900) specifies the number of milliseconds it takes to scroll to the specified area
      $('html, body').animate({
        scrollTop: $(hash).offset().top
      }, 900, function(){

        // Add hash (#) to URL when done scrolling (default click behavior)
        window.location.hash = hash;
      });
    } // End if
  });

  $(window).scroll(function() {
    $(".slideanim").each(function(){
      var pos = $(this).offset().top;

      var winTop = $(window).scrollTop();
        if (pos < winTop + 600) {
          $(this).addClass("slide");
        }
    });
  });
})
</script>

</body>
</html>
